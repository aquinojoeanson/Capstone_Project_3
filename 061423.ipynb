{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d3ed6d",
   "metadata": {},
   "source": [
    "### DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691a124",
   "metadata": {},
   "source": [
    "The Amazon dataset contains the customer reviews for all listed Electronics products spanning from May 1996 up to July 2014.There are a total of 1,689,188 reviews by a total of 192,403 customers on 63,001 unique products. The data dictionary is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be382e4",
   "metadata": {},
   "source": [
    "* asin - Unique ID of the product being reviewed, string\n",
    "* helpful - A list with two elements: the number of users that voted helpful, and the total number of users that voted on the review (including the not helpful votes), list\n",
    "* overall - The reviewer's rating of the product, int64\n",
    "* reviewText - The review text itself, string\n",
    "* reviewerID - Unique ID of the reviewer, string\n",
    "* reviewerName - Specified name of the reviewer, string\n",
    "* summary - Headline summary of the review, string\n",
    "* unixReviewTime - Unix Time of when the review was posted, string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17683773",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e180b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497afdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "731c69fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aquinojoeanson/Desktop/SPRINGBOARD/Capstone_Project_3/Notebook\n"
     ]
    }
   ],
   "source": [
    "#Get the current working directory of a process and print \n",
    "MyWorkingDir = os.getcwd()\n",
    "\n",
    "print(MyWorkingDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739af0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and read the Dataset reviews_Electronics_5.json on woking directory\n",
    "# and fed into DataFrame or df\n",
    "df = pd.read_json('/Users/aquinojoeanson/Desktop/SPRINGBOARD/Capstone_Project_3/Notebook/reviews_Electronics_5.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d30195d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>5</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1370131200</td>\n",
       "      <td>06 2, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1290643200</td>\n",
       "      <td>11 25, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1283990400</td>\n",
       "      <td>09 9, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>11 24, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>09 29, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A2JXAZZI9PHK9Z</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>Billy G. Noland \"Bill Noland\"</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>I am using this with a Nook HD+. It works as d...</td>\n",
       "      <td>5</td>\n",
       "      <td>HDMI Nook adapter cable</td>\n",
       "      <td>1388707200</td>\n",
       "      <td>01 3, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A2P5U7BDKKT7FW</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>Christian</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>The cable is very wobbly and sometimes disconn...</td>\n",
       "      <td>2</td>\n",
       "      <td>Cheap proprietary scam</td>\n",
       "      <td>1398556800</td>\n",
       "      <td>04 27, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAZ084UMH8VZ2</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>D. L. Brown \"A Knower Of Good Things\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This adaptor is real easy to setup and use rig...</td>\n",
       "      <td>5</td>\n",
       "      <td>A Perfdect Nook HD+ hook up</td>\n",
       "      <td>1399161600</td>\n",
       "      <td>05 4, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AEZ3CR6BKIROJ</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>Mark Dietter</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This adapter easily connects my Nook HD 7&amp;#34;...</td>\n",
       "      <td>4</td>\n",
       "      <td>A nice easy to use accessory.</td>\n",
       "      <td>1405036800</td>\n",
       "      <td>07 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>Matenai</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>This product really works great but I found th...</td>\n",
       "      <td>5</td>\n",
       "      <td>This works great but read the details...</td>\n",
       "      <td>1390176000</td>\n",
       "      <td>01 20, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                           reviewerName  \\\n",
       "0   AO94DHGC771SJ  0528881469                                amazdnu   \n",
       "1   AMO214LNFCEI4  0528881469                        Amazon Customer   \n",
       "2  A3N7T0DY83Y4IG  0528881469                          C. A. Freeman   \n",
       "3  A1H8PY3QHMQQA0  0528881469               Dave M. Shaw \"mack dave\"   \n",
       "4  A24EV6RXELQZ63  0528881469                            Wayne Smith   \n",
       "5  A2JXAZZI9PHK9Z  0594451647          Billy G. Noland \"Bill Noland\"   \n",
       "6  A2P5U7BDKKT7FW  0594451647                              Christian   \n",
       "7   AAZ084UMH8VZ2  0594451647  D. L. Brown \"A Knower Of Good Things\"   \n",
       "8   AEZ3CR6BKIROJ  0594451647                           Mark Dietter   \n",
       "9  A3BY5KCNQZXV5U  0594451647                                Matenai   \n",
       "\n",
       "    helpful                                         reviewText  overall  \\\n",
       "0    [0, 0]  We got this GPS for my husband who is an (OTR)...        5   \n",
       "1  [12, 15]  I'm a professional OTR truck driver, and I bou...        1   \n",
       "2  [43, 45]  Well, what can I say.  I've had this unit in m...        3   \n",
       "3   [9, 10]  Not going to write a long review, even thought...        2   \n",
       "4    [0, 0]  I've had mine for a year and here's what we go...        1   \n",
       "5    [3, 3]  I am using this with a Nook HD+. It works as d...        5   \n",
       "6    [0, 0]  The cable is very wobbly and sometimes disconn...        2   \n",
       "7    [0, 0]  This adaptor is real easy to setup and use rig...        5   \n",
       "8    [0, 0]  This adapter easily connects my Nook HD 7&#34;...        4   \n",
       "9    [3, 3]  This product really works great but I found th...        5   \n",
       "\n",
       "                                    summary  unixReviewTime   reviewTime  \n",
       "0                           Gotta have GPS!      1370131200   06 2, 2013  \n",
       "1                         Very Disappointed      1290643200  11 25, 2010  \n",
       "2                            1st impression      1283990400   09 9, 2010  \n",
       "3                   Great grafics, POOR GPS      1290556800  11 24, 2010  \n",
       "4    Major issues, only excuses for support      1317254400  09 29, 2011  \n",
       "5                   HDMI Nook adapter cable      1388707200   01 3, 2014  \n",
       "6                    Cheap proprietary scam      1398556800  04 27, 2014  \n",
       "7               A Perfdect Nook HD+ hook up      1399161600   05 4, 2014  \n",
       "8             A nice easy to use accessory.      1405036800  07 11, 2014  \n",
       "9  This works great but read the details...      1390176000  01 20, 2014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overall and the unixReviewTime only stored as integers. The rest are interpreted as strings.\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43b717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1689188 entries, 0 to 1689187\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   reviewerID      1689188 non-null  object\n",
      " 1   asin            1689188 non-null  object\n",
      " 2   reviewerName    1664458 non-null  object\n",
      " 3   helpful         1689188 non-null  object\n",
      " 4   reviewText      1689188 non-null  object\n",
      " 5   overall         1689188 non-null  int64 \n",
      " 6   summary         1689188 non-null  object\n",
      " 7   unixReviewTime  1689188 non-null  int64 \n",
      " 8   reviewTime      1689188 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 116.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a387372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting unixReviewTime to Unix time, so it will be more accurate when the review was posted\n",
    "from datetime import datetime\n",
    "\n",
    "condition = lambda row: datetime.fromtimestamp(row).strftime(\"%m-%d-%Y\")\n",
    "df[\"unixReviewTime\"] = df[\"unixReviewTime\"].apply(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc956fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>5</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>06-01-2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>11-24-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>09-08-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>11-23-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>1</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>09-28-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin              reviewerName   helpful  \\\n",
       "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
       "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
       "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
       "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
       "4  A24EV6RXELQZ63  0528881469               Wayne Smith    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...        5   \n",
       "1  I'm a professional OTR truck driver, and I bou...        1   \n",
       "2  Well, what can I say.  I've had this unit in m...        3   \n",
       "3  Not going to write a long review, even thought...        2   \n",
       "4  I've had mine for a year and here's what we go...        1   \n",
       "\n",
       "                                  summary unixReviewTime  \n",
       "0                         Gotta have GPS!     06-01-2013  \n",
       "1                       Very Disappointed     11-24-2010  \n",
       "2                          1st impression     09-08-2010  \n",
       "3                 Great grafics, POOR GPS     11-23-2010  \n",
       "4  Major issues, only excuses for support     09-28-2011  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop(labels=\"reviewTime\", axis=1, inplace=True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c12b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got this GPS for my husband who is an (OTR) over the road trucker.  Very Impressed with the shipping time, it arrived a few days earlier than expected...  within a week of use however it started freezing up... could of just been a glitch in that unit.  Worked great when it worked!  Will work great for the normal person as well but does have the \"trucker\" option. (the big truck routes - tells you when a scale is coming up ect...)  Love the bigger screen, the ease of use, the ease of putting addresses into memory.  Nothing really bad to say about the unit with the exception of it freezing which is probably one in a million and that's just my luck.  I contacted the seller and within minutes of my email I received a email back with instructions for an exchange! VERY impressed all the way around!\n"
     ]
    }
   ],
   "source": [
    "#Sample product review (string) in reviewText\n",
    "print(df[\"reviewText\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "776b0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 3 2 4]\n"
     ]
    }
   ],
   "source": [
    "# On overall field is associated with reviews rating, will use it as the base truth labels for the model as a quantified summary.\n",
    "print(df.overall.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad516fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to add wireless audio streaming to your home theater or home stereo that has 3.5mm input? Maybe you want to stream music in your car from your Android or iPhone? This little beauty adds wireless capability to a wireless-incapable device with ease.The only caveat is that it needs a USB power connection to run. This is good if you have a spare charger and a spare power outlet, bad if you don't. It's not a big deal to work around, but it is something you will want to keep in mind.Connecting to the device is easy. Just plug it into power, let it power on. Connect the 3.5mm output to your audio setup with a 3.5mm input. On your phone or Wi-Fi enabled audio player, turn on Wi-Fi. Search for the Sabrent_A1AE and connect. Now fire up your music app, such as Music on the iPhone, and play away. Well, make sure your stereo is powered up and on the right input (or car stereo or surround receiver and so on).The nice thing about this is that it uses Wi-Fi. This means it doesn't have the compression issues you might hear or experience on a Bluetooth connection.Another option you can use is to create fake wireless headphones. This is kind of a joke of an option, as you need a power source, but if you want to cope with it, this can be done. I tried plugging this into one of the myriad of &#34;power banks&#34; I have. It powers up and allows you to connect. Once connected, plug in headphones and now you have headphones that are receiving wireless audio over a completely wired connection on the receiving end. It works, but not really what this seems to have been designed for.Another caveat you might have is that with the way the device is designed, you can't really have more than one of these in the house and connected at one time. The problem is that they all use the same wireless network name and static IP address, meaning you will never be sure which you are connecting to. Trial and error I guess. As I don't have the need for this, it's not a big deal for me, but for some it might be a deal breaker.Works well, sounds good and is easy to configure.Recommended.I received a free sample for review purposes from Sabrent.\n"
     ]
    }
   ],
   "source": [
    "# NLP Pre-Processing... Original Form\n",
    "sample_review = df[\"reviewText\"].iloc[1689160]\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59af0f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to add wireless audio streaming to your home theater or home stereo that has 3.5mm input? Maybe you want to stream music in your car from your Android or iPhone? This little beauty adds wireless capability to a wireless-incapable device with ease.The only caveat is that it needs a USB power connection to run. This is good if you have a spare charger and a spare power outlet, bad if you don't. It's not a big deal to work around, but it is something you will want to keep in mind.Connecting to the device is easy. Just plug it into power, let it power on. Connect the 3.5mm output to your audio setup with a 3.5mm input. On your phone or Wi-Fi enabled audio player, turn on Wi-Fi. Search for the Sabrent_A1AE and connect. Now fire up your music app, such as Music on the iPhone, and play away. Well, make sure your stereo is powered up and on the right input (or car stereo or surround receiver and so on).The nice thing about this is that it uses Wi-Fi. This means it doesn't have the compression issues you might hear or experience on a Bluetooth connection.Another option you can use is to create fake wireless headphones. This is kind of a joke of an option, as you need a power source, but if you want to cope with it, this can be done. I tried plugging this into one of the myriad of \"power banks\" I have. It powers up and allows you to connect. Once connected, plug in headphones and now you have headphones that are receiving wireless audio over a completely wired connection on the receiving end. It works, but not really what this seems to have been designed for.Another caveat you might have is that with the way the device is designed, you can't really have more than one of these in the house and connected at one time. The problem is that they all use the same wireless network name and static IP address, meaning you will never be sure which you are connecting to. Trial and error I guess. As I don't have the need for this, it's not a big deal for me, but for some it might be a deal breaker.Works well, sounds good and is easy to configure.Recommended.I received a free sample for review purposes from Sabrent.\n"
     ]
    }
   ],
   "source": [
    "# Reversing the HTML to original character presentation.\n",
    "import html\n",
    "\n",
    "decoded_review = html.unescape(sample_review)\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "418eaa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want to add wireless audio streaming to your home theater or home stereo that has 3.5mm input? Maybe you want to stream music in your car from your Android or iPhone? This little beauty adds wireless capability to a wireless-incapable device with ease.The only caveat is that it needs a USB power connection to run. This is good if you have a spare charger and a spare power outlet, bad if you don't. It's not a big deal to work around, but it is something you will want to keep in mind.Connecting to the device is easy. Just plug it into power, let it power on. Connect the 3.5mm output to your audio setup with a 3.5mm input. On your phone or Wi-Fi enabled audio player, turn on Wi-Fi. Search for the Sabrent_A1AE and connect. Now fire up your music app, such as Music on the iPhone, and play away. Well, make sure your stereo is powered up and on the right input (or car stereo or surround receiver and so on).The nice thing about this is that it uses Wi-Fi. This means it doesn't have the compression issues you might hear or experience on a Bluetooth connection.Another option you can use is to create fake wireless headphones. This is kind of a joke of an option, as you need a power source, but if you want to cope with it, this can be done. I tried plugging this into one of the myriad of power banks I have. It powers up and allows you to connect. Once connected, plug in headphones and now you have headphones that are receiving wireless audio over a completely wired connection on the receiving end. It works, but not really what this seems to have been designed for.Another caveat you might have is that with the way the device is designed, you can't really have more than one of these in the house and connected at one time. The problem is that they all use the same wireless network name and static IP address, meaning you will never be sure which you are connecting to. Trial and error I guess. As I don't have the need for this, it's not a big deal for me, but for some it might be a deal breaker.Works well, sounds good and is easy to configure.Recommended.I received a free sample for review purposes from Sabrent.\n"
     ]
    }
   ],
   "source": [
    "# Removing the punctuation marks, it has no value on NLP.\n",
    "pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "df[\"preprocessed\"] = df[\"reviewText\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302bb68",
   "metadata": {},
   "source": [
    "# Extracting the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e48994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linking together as a string whitespace.\n",
    "#%%time\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#import nltk resources\n",
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "#apply our functions\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing accents \n",
    "from unicodedata import normalize\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_accent)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing puntuations, keeping whitespace and alphanumeric only. \n",
    "pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted to lower case\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.lower()\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfabeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "print(f\"sample stop words: {stop_words[:15]} \\n\")\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") \\\n",
    "                                          if token not in stop_words])\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_stop_words)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97954063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Extra Space, To ensure no more than one single whitespace in the sentence.\n",
    "pattern = r\"[\\s]+\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1689160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55dd832",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting each review and transfored into a list of words.\n",
    "corpora = df[\"preprocessed\"].values\n",
    "tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "\n",
    "print(tokenized[1689160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9707e",
   "metadata": {},
   "source": [
    "# Phrase Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4129d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting atleast 300 time that two words appear.\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bi_gram = Phrases(tokenized, min_count=300, threshold=50)\n",
    "\n",
    "tri_gram = Phrases(bi_gram[tokenized], min_count=300, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161c6ef",
   "metadata": {},
   "source": [
    "# Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8780258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single pieces of token.\n",
    "uni_gram_tokens = set([token for text in tokenized for token in text])\n",
    "uni_gram_tokens = set(filter(lambda x: x != \"\", uni_gram_tokens))\n",
    "\n",
    "print(list(uni_gram_tokens)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8638d3",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From gensim phaser filtering bi_gram phrases.\n",
    "bigram_min = bi_gram.min_count\n",
    "\n",
    "bi_condition = lambda x: x[1] >= bigram_min\n",
    "\n",
    "bi_gram_tokens = dict(filter(bi_condition, bi_gram.vocab.items()))\n",
    "bi_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                      for token in bi_gram_tokens])\n",
    "\n",
    "bi_grams_only = bi_gram_tokens.difference(uni_gram_tokens)\n",
    "print(list(bi_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e5fb2",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2af421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linking bi_gram plus adjacent tokens.\n",
    "trigram_min = tri_gram.min_count\n",
    "\n",
    "tri_condition = lambda x: x[1] >= trigram_min\n",
    "\n",
    "tri_gram_tokens = dict(filter(tri_condition, tri_gram.vocab.items()))\n",
    "tri_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                       for token in tri_gram_tokens])\n",
    "\n",
    "tri_grams_only = tri_gram_tokens.difference(bi_gram_tokens)\n",
    "print(list(tri_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri_gram and bi_gram phrasers are applied to our tokenized corpora.\n",
    "tokenized = [Phraser(tri_gram)[Phraser(bi_gram)[i]] for i in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3462310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final form, Single character are removed.\n",
    "tokenized = [list(filter(lambda x: len(x) > 1, document)) \\\n",
    "             for document in tokenized]\n",
    "\n",
    "print(tokenized[1689160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53b23a",
   "metadata": {},
   "source": [
    "# Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45925003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens are assigned to lookup ID.\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "vocabulary = Dictionary(tokenized)\n",
    "\n",
    "vocabulary_keys = list(vocabulary.token2id)[0:10]\n",
    "\n",
    "for key in vocabulary_keys:\n",
    "    print(f\"ID: {vocabulary.token2id[key]}, Token: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c7445",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fd8a9",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numerical values. \n",
    "# Counting how many times a word appears.\n",
    "bow = [vocabulary.doc2bow(doc) for doc in tokenized]\n",
    "\n",
    "for idx, freq in bow[0]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b6c61",
   "metadata": {},
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighting if lower or higher based on our bow variable.\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "for idx, weight in tfidf[bow[0]]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Weight: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086d9d1",
   "metadata": {},
   "source": [
    "# Word Embedding for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding token in the Word2vec model.\n",
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "feature_size = 100\n",
    "context_size = 20\n",
    "min_word = 1\n",
    "\n",
    "word_vec= word2vec.Word2Vec(tokenized, size=feature_size, \\\n",
    "                            window=context_size, min_count=min_word, \\\n",
    "                            iter=50, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abace4",
   "metadata": {},
   "source": [
    "# Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering all unique token using word_vec model. \n",
    "word_vec_unpack = [(word, idx.index) for word, idx in \\\n",
    "                   word_vec.wv.vocab.items()]\n",
    "\n",
    "tokens, indexes = zip(*word_vec_unpack)\n",
    "\n",
    "# word_vec_df is sliced by words\n",
    "word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)\n",
    "\n",
    "display(word_vec_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_array = np.array(tokenized)\n",
    "\n",
    "# model_array shape is therefore the word count on axis 0 and the number of dimensions on axis 1.\n",
    "model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dae199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df final DF\n",
    "model_df = pd.DataFrame(model_array)\n",
    "model_df[\"label\"] = df[\"overall\"]\n",
    "\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2f919",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use on our model_df to reduce its 100 dimentions to 2 dimentions.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#sampling the model_df population\n",
    "pca_df = model_df.reset_index()\n",
    "pca_df = model_df.dropna(axis=0).iloc[:,1:]\n",
    "pca_df = pca_df.iloc[::50]\n",
    "\n",
    "#setting up PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca = pca.fit_transform(pca_df.iloc[:, :-1])\n",
    "labels = pca_df[\"label\"]\n",
    "\n",
    "#setting up plot components\n",
    "x_axis = pca[:,0]\n",
    "y_axis = pca[:,1]\n",
    "color_map = pca_df[\"label\"].map({1:\"blue\", \\\n",
    "                                 2:\"red\", \\\n",
    "                                 3:\"yellow\", \\\n",
    "                                 4:\"green\", \\\n",
    "                                 5:\"orange\"})\n",
    "\n",
    "#plotting PCA\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "plt.scatter(x_axis, y_axis, color=color_map, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e0f47",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0eb5b",
   "metadata": {},
   "source": [
    "# Word2Vec More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db35932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll implement several interesting Natural Language Processing techniques in order to explore our Amazon dataset.\n",
    "# Taking five common words in our corpora and using word_vec we derive their five most related words. \n",
    "word_bank = [\"nook\", \"phone\", \"tv\", \"good\", \"price\"]\n",
    "\n",
    "for word in word_bank[:]:\n",
    "    related_vec = word_vec.wv.most_similar(word, topn=5)\n",
    "    related_words = np.array(related_vec)[:,0]\n",
    "    word_bank.extend(related_words)\n",
    "    print(f\"{word}: {related_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54e22e",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assists in visualizing high-dimensional dataset.\n",
    "# will provide coordinates of each word in a 2D scatterplot plane.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=5, n_iter=1000, random_state=42)\n",
    "\n",
    "sample_vecs = word_vec.wv[set(word_bank)]\n",
    "sample_tsne = tsne.fit_transform(sample_vecs)\n",
    "tsne_x = sample_tsne[:, 0]\n",
    "tsne_y = sample_tsne[:, 1]\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = plt.scatter(x=tsne_x, y=tsne_y)\n",
    "\n",
    "for label, x, y in zip(word_bank, tsne_x, tsne_y):\n",
    "    plt.annotate(label, xy=(x+3, y+3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373bf26",
   "metadata": {},
   "source": [
    "# Vector Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f47b3",
   "metadata": {},
   "source": [
    "add (combine the meaning of the components) or \n",
    "subtract (to take out the context of one token from another) word vectors together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c5579",
   "metadata": {},
   "source": [
    "### Example 1: Books + Touchscreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector algebra and similarity scores\n",
    "word_vec.wv.most_similar(positive=[\"books\", \"touchscreen\"], \\\n",
    "                      negative=[], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7a7a7",
   "metadata": {},
   "source": [
    "### Example 2: Cheap – Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector algebra and similarity scores\n",
    "word_vec.wv.most_similar(positive=[\"cheap\"], \\\n",
    "                      negative=[\"quality\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea2d8f",
   "metadata": {},
   "source": [
    "### Example 3: Tablet – Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6377d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector algebra and similarity scores\n",
    "word_vec.wv.most_similar(positive=[\"tablet\"], \\\n",
    "                      negative=[\"phone\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d209f66e",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_helpful_text is the highest-rated product review by Amazon users.\n",
    "# 1st element storing the number of helful votes\n",
    "# second element containing  the total number of helpful and not helpful review votes.\n",
    "helpful = df[\"helpful\"].tolist()\n",
    "most_helpful = max(helpful, key=lambda x: x[0])\n",
    "\n",
    "most_helpful_idx = df[\"helpful\"].astype(str) == str(most_helpful)\n",
    "most_helpful_idx = df[most_helpful_idx].index\n",
    "\n",
    "most_helpful_text = df[\"reviewText\"].iloc[most_helpful_idx].values[0]\n",
    "\n",
    "print(most_helpful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee981dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go further and identify what nouns in th documents refer to using NER(noun tagging)\n",
    "#ner_dict, a dictionary initialized as a list, to segregate the nouns in the most_helpful_text into the NER tags\n",
    "%%time\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "ner = spacy.load(\"en\")\n",
    "\n",
    "ner_helpful = ner(most_helpful_text)\n",
    "\n",
    "ner_dict = defaultdict(list)\n",
    "for entity in ner_helpful.ents:\n",
    "    ner_dict[entity.label_].append(entity)\n",
    "\n",
    "for NER, name in ner_dict.items():\n",
    "    print(f\"{NER}:\\n{name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the tags in the review\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(ner_helpful, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1801e5",
   "metadata": {},
   "source": [
    "# Dependency Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciphering by breaking down and influending each tokens\n",
    "# dependency trees of the first three sentences of the most_helpful_text.\n",
    "def ner_displacy(sentence):\n",
    "    ner_sentence = ner(sentence)\n",
    "    displacy.render(ner_sentence, jupyter=True, \\\n",
    "                    options={\"compact\": False, \\\n",
    "                             \"distance\": 90, \\\n",
    "                             \"word_spacing\":20, \\\n",
    "                             \"arrow_spacing\":10, \\\n",
    "                             \"arrow_stroke\": 2, \\\n",
    "                             \"arrow_width\": 5})\n",
    "\n",
    "for sentence in most_helpful_text.split(\".\")[0:3]:\n",
    "    ner_displacy(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b4866",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews can be classified and grouped according to the type of electronics product they correspond to.\n",
    "# product reviews will assigned weight to the topic\n",
    "# topics will have weights on token.\n",
    "# top five words that are salient to the first group of product reviews.\n",
    "\n",
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "num_topics = 10\n",
    "bow_lda = LdaMulticore(bow, num_topics=num_topics, id2word=vocabulary, \\\n",
    "                       passes=5, workers=cores, random_state=42)\n",
    "\n",
    "for token, frequency in bow_lda.show_topic(0, topn=5):\n",
    "    print(token, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the data \n",
    "# calling upon each word group\n",
    "for topic in range(0, num_topics):\n",
    "    print(f\"\\nTopic {topic+1}:\")\n",
    "    for token, frequency in bow_lda.show_topic(topic, topn=5):\n",
    "        print(f\" {token}, {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad627fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  interactively explore the words associated with the topics derived by LDA\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_idm = pyLDAvis.gensim.prepare(bow_lda, bow, vocabulary)\n",
    "\n",
    "pyLDAvis.display(lda_idm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68de0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
